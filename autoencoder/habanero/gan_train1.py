# -*- coding: utf-8 -*-
"""GAN_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cf5TjVuX2qnH6LiTglAabNTK2qtrYlQS
"""

## This script is based on the Tensorflow tutorial for DCGAN

# step 0. load packages
import tensorflow as tf
#tf.enable_eager_execution()

#import glob
#import imageio
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
#import os
#import PIL
import time
import cv2
import math

#from IPython import display

# step 1. load image

# from google.colab import drive
# drive.mount('/content/drive')

# %cd 'drive/My Drive/TZStats - Summer 2019/APL-Helen/code'


# img = cv2.imread('../data/sample/shadow_remove.jpg')
img = cv2.imread('shadow_remove.jpg')

# img = img[:5000, :5000, :]
n = math.ceil(img.shape[0] / 100) # note that we're dealing with a n x n [100px x 100px] patches

## normalize training images 
train_images = np.zeros((n*n, 96, 96, 3), dtype=np.float32)
for i in range(n*n):
    #print(i)
    rr = i // n
    cc = i - rr * n
    #print(rr, cc)
    train_images[i] = img[(rr*96):(rr*96+96), (cc*96):(cc*96+96)]


# plt.imshow(train_images[0])

train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]
print('training images:', train_images.shape)

## set buffer size and batch size
BUFFER_SIZE = 10000
BATCH_SIZE = 64

## create batches
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

# step 3. build models
## step 3.1. function definitions
## part 1. generator model
def make_generator_model():
    model = tf.keras.Sequential()
    ## layer 1: fully-connected + batch normalization + leaky relu activation 
    model.add(tf.keras.layers.Dense(6*6*256, use_bias=False, input_shape=(200,))) ## input dimension: 100-dim vector 
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())
    ## reshape the layer
    model.add(tf.keras.layers.Reshape((6, 6, 256)))
    assert model.output_shape == (None, 6, 6, 256) # Note: Assert just check the conditions; None is the batch size
    
    ## layer 2: upsampling layer
    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 6, 6, 128)  
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    ## layer 3: upsampling layer
    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 12, 12, 128)    
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())
    
    ## layer 4: upsampling layer
    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 24, 24, 64)    
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())
    
    ## layer 5: upsampling layer
    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 48, 48, 64)    
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())
    
    ## layer 6: upsampling layer
    model.add(tf.keras.layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 96, 96, 3)
  
    return model

## part 2. discriminator model
def make_discriminator_model():
    model = tf.keras.Sequential()
    ## layer 1: convolutional layer
    model.add(tf.keras.layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same', input_shape=(96,96,3)))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dropout(0.3))
      
    ## layer 2: convolutional layer
    model.add(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dropout(0.3))
    
    ## layer 3: convolutional layer
    model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dropout(0.3))
    
    ## layer 4: convolutional layer
    model.add(tf.keras.layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same'))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dropout(0.3))
    
    ## layer 5: fully connected layer (after flatten)   
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(1))
     
    return model

## step 3.2. initialize models
generator = make_generator_model()
discriminator = make_discriminator_model()

# step 4. define loss function and optimizer
## step 4.1. loss function definition
## part 1. generator loss
def generator_loss(generated_output):
    return tf.losses.sigmoid_cross_entropy(tf.ones_like(generated_output), generated_output)

## part 2. discriminator loss
def discriminator_loss(real_output, generated_output):
    # [1,1,...,1] with real output since it is true and we want our generated examples to look like it
    real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.ones_like(real_output), logits=real_output)

    # [0,0,...,0] with generated images since they are fake
    generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.zeros_like(generated_output), logits=generated_output)

    total_loss = real_loss + generated_loss

    return total_loss

## step 4.2. define optimizer
generator_optimizer = tf.train.AdamOptimizer(1e-4)
discriminator_optimizer = tf.train.AdamOptimizer(1e-4)

## step 4.3. define checkpoints
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

# step 5. set up GANs
## step 5.1. define training parameters
EPOCHS = 100
noise_dim = 200
num_examples_to_generate = 16

# We'll re-use this random vector used to seed the generator so
# it will be easier to see the improvement over time.
random_vector_for_generation = tf.random_normal([num_examples_to_generate,
                                                 noise_dim])

## step 5.2. define training methods
## part 1. define training step
def train_step(images):
    ## generating noise from a normal distribution
    noise = tf.random_normal([BATCH_SIZE, noise_dim])
    ## tape gradient
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)
        
        real_output = discriminator(images, training=True)
        generated_output = discriminator(generated_images, training=True)
        
        gen_loss = generator_loss(generated_output)
        disc_loss = discriminator_loss(real_output, generated_output)
    
    gradients_of_generator = gen_tape.gradient(gen_loss, generator.variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.variables)
      
    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.variables))

## defunctionalize eager execution for train_step
train_step = tf.contrib.eager.defun(train_step)

## part 2. define train process
def train(dataset, epochs):
    for epoch in range(epochs):
        start = time.time()

        for images in dataset:
            train_step(images)

        display.clear_output(wait=True)
        generate_and_save_images(generator, epoch + 1, random_vector_for_generation)

        ## saving checkpoints every 50 epochs
        if (epoch + 1) % 50 == 0:
            checkpoint.save(file_prefix = checkpoint_prefix)
        
        print('Time taken for epoch {} is {} sec'.format(epoch + 1, time.time() - start))
    
    print("done!")
    ## generating after final epoch
    display.clear_output(wait=True)
    generate_and_save_images(generator, epochs, random_vector_for_generation)

## part 3. generate and save images
def generate_and_save_images(model, epoch, test_input):
    # make sure the training parameter is set to False because we
    # don't want to train the batchnorm layer when doing inference.
    predictions = model(test_input, training=False)
    
    fig = plt.figure(figsize=(4, 4))
    
    for i in range(predictions.shape[0]):
        plt.subplot(4, 4, i+1)
        img_show = np.array(predictions[i, :, :] * 127.5 + 127.5)
        img_show = img_show.astype(np.uint8)
        b,g,r = cv2.split(img_show)
        img_show = cv2.merge([r,g,b])
        plt.imshow(img_show)
        plt.axis('off')
    
    plt.savefig('output/image_at_epoch_{:04d}.png'.format(epoch)) # note that this assumes output folder exists
    plt.show()

# step 6. train GANs
train(train_dataset, EPOCHS)

# note that this has never finished in Colab... nor do we really expect it to? (unfortunate to say)
# we're just using this to test the waters, will use Habanero for more computing power

# step 7. generate images
## restore the model using the most recent checkpoint
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

z_vec = np.random.normal(size=[300, 200])
img_output = generator(z_vec, training=False)
img_output = np.array(img_output)

for i in range(300):
    img_show = img_output[i] * 127.5 + 127.5
    img_show = img_show.astype(np.uint8)
    #b,g,r = cv2.split(img_show)
    #img_show = cv2.merge([r,g,b])
    file_path = 'output/' + '{0:04}'.format(i+1) + '.jpg'
#     print(file_path)
#     print(img_show)
    cv2.imwrite(file_path, img_show)


















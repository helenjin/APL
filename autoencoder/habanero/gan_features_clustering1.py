# -*- coding: utf-8 -*-
"""GAN_features_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FRE1BmTgEfmhkPxGRSrySObxbn4rtA7d
"""

## This script is for feature extraction of DCGAN discriminator

# step 0. load packages
import tensorflow as tf
# tf.enable_eager_execution()

# import glob
# import imageio
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
# import PIL
import time
import cv2
import math

from IPython import display

# step 1. load image

# from google.colab import drive
# drive.mount('/content/drive')

# %cd 'drive/My Drive/TZStats - Summer 2019/APL-Helen/code'

img = cv2.imread('shadow_remove.jpg')

# img = img[:5000, :5000, :]
n = math.ceil(img.shape[0] / 100) # note that we're dealing with a n x n [100px x 100px] patches

## normalize training images 
train_images = np.zeros((n*n, 96, 96, 3), dtype=np.float32)
for i in range(n*n):
    #print(i)
    rr = i // n
    cc = i - rr * n
    #print(rr, cc)
    train_images[i] = img[(rr*96):(rr*96+96), (cc*96):(cc*96+96)]
    
train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]
print('training images:', train_images.shape)

# step 3. build models
## step 3.1. function definitions
## part 1. generator model
def make_generator_model():
    model = tf.keras.Sequential()
    ## layer 1: fully-connected + batch normalization + leaky relu activation 
    model.add(tf.keras.layers.Dense(6*6*256, use_bias=False, input_shape=(200,))) ## input dimension: 100-dim vector 
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())
    ## reshape the layer
    model.add(tf.keras.layers.Reshape((6, 6, 256)))
    assert model.output_shape == (None, 6, 6, 256) # Note: Assert just check the conditions; None is the batch size
    
    ## layer 2: upsampling layer
    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 6, 6, 128)  
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    ## layer 3: upsampling layer
    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 12, 12, 128)    
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())
    
    ## layer 4: upsampling layer
    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 24, 24, 64)    
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())
    
    ## layer 5: upsampling layer
    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 48, 48, 64)    
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())
    
    ## layer 6: upsampling layer
    model.add(tf.keras.layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 96, 96, 3)
  
    return model

## part 2. discriminator model
def make_discriminator_model():
    model = tf.keras.Sequential()
    ## layer 1: convolutional layer
    model.add(tf.keras.layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same', input_shape=(96,96,3)))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dropout(0.3))
      
    ## layer 2: convolutional layer
    model.add(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dropout(0.3))
    
    ## layer 3: convolutional layer
    model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dropout(0.3))
    
    ## layer 4: convolutional layer
    model.add(tf.keras.layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same'))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dropout(0.3))
    
    ## layer 5: fully connected layer (after flatten)   
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(1))
     
    return model

## step 3.2. initialize models
generator = make_generator_model()
discriminator = make_discriminator_model()

# step 4. define loss function and optimizer
## step 4.1. loss function definition
## part 1. generator loss
def generator_loss(generated_output):
    return tf.losses.sigmoid_cross_entropy(tf.ones_like(generated_output), generated_output)

## part 2. discriminator loss
def discriminator_loss(real_output, generated_output):
    # [1,1,...,1] with real output since it is true and we want our generated examples to look like it
    real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.ones_like(real_output), logits=real_output)

    # [0,0,...,0] with generated images since they are fake
    generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.zeros_like(generated_output), logits=generated_output)

    total_loss = real_loss + generated_loss

    return total_loss

## step 4.2. define optimizer
generator_optimizer = tf.train.AdamOptimizer(1e-4)
discriminator_optimizer = tf.train.AdamOptimizer(1e-4)

## step 4.3. define checkpoints
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

# step 5. load pretrained model
# **we assume we've already run GAN_train notebook**
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

# step 7. generate new images
z_vec = np.random.normal(size=[300, 200])
img_output = generator(z_vec, training=False)
img_output = np.array(img_output)

for i in range(300):
    img_show = img_output[i] * 127.5 + 127.5
    img_show = img_show.astype(np.uint8)
    #b,g,r = cv2.split(img_show)
    #img_show = cv2.merge([r,g,b])
    file_path = 'new/' + '{0:04}'.format(i+1) + '.jpg'
    cv2.imwrite(file_path, img_show)

# step 8. generate new features
from tensorflow.keras.models import Model
intermediate_layer_model = Model(inputs=discriminator.input, outputs=discriminator.get_layer('flatten').output)

feat_cnn = np.zeros([train_images.shape[0], 1152])
for i in range(math.ceil(train_images.shape[0]/100)): 
    feat_cnn[(i*100):(i*100+100)] = intermediate_layer_model.predict(train_images[(i*100):(i*100+100)]) 
    
# I think this should work? just that I never finished my training on the GAN_train notebook (due to colab's limits)
# so I want to guess that the predict will actual return something substantial if the model training is done as it's supposed to be

# step 9. clustering over CNN features
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import MiniBatchKMeans

## transform data
#X = StandardScaler().fit_transform(feat_cnn)
X = feat_cnn
## set #cluster = 20
n_c = 20
from datetime import datetime
stt = datetime.now()
## run minibatch kmeans over training set
kmeans = MiniBatchKMeans(n_clusters=n_c, random_state=2019, 
                         batch_size=8000, max_iter=5000, max_no_improvement=None)
kmeans = kmeans.fit(X)
print('Time for clustering:', datetime.now() - stt)

# step 10. visualize clusters
labels = kmeans.predict(X)
im_rgb = np.zeros((img.shape[0],img.shape[1],3), dtype=np.uint8)
gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
for id_c in range(n_c):
    print('cluster id is', id_c)
    im_rgb[:, :, 0] = gray_image
    im_rgb[:, :, 1] = gray_image
    im_rgb[:, :, 2] = gray_image
    loc_c = np.where(labels == id_c)[0]
    print(loc_c.shape)
    for i in loc_c:
    ## choose patch
    #img = np.zeros((100, 100, 3), dtype=np.uint8)
        n_row = int(i/100)
        n_col = i - n_row*100
        n_1 = n_row*100
        n_2 = 100*(n_row + 1)
        m_1 = n_col*100
        m_2 = 100*(n_col + 1)
        im_rgb[n_1:n_2, m_1:m_2] = img[n_1:n_2, m_1:m_2]
    img_path = 'clusters/cluster_' + str(id_c) + '.jpg'
    #print(img_path)
    cv2.imwrite(img_path,im_rgb)
    print('cluster ', id_c, ' is saved!')

